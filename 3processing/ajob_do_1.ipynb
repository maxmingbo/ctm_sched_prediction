{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_list= ['180602', '180603', '180604', '180605', '180606', '180607', '180608', '180609', '180610',\\\n",
    "#           '180611', '180612', '180613', '180614', '180615', '180616', '180617', '180618', '180619', '180620',\\\n",
    "#           '180621', '180622', '180623', '180624', '180625', '180626', '180627', '180628', '180629', '180630',\\\n",
    "#           '180701', '180702', '180703', '180704', '180705', '180706', '180707', '180708', '180709', '180710', \\\n",
    "#           '180711', '180712', '180713', '180714', '180715']\n",
    "# d_list = ['180716', '180717', '180718', '180719', '180720', '180721', '180722', '180723', '180724', '180725', \\\n",
    "#           '180726', '180727', '180728', '180729', '180730', '180731', '180801', '180802', '180803', '180804', \\\n",
    "#           '180805', '180806', '180807', '180808', '180809', '180810', '180811', '180812', '180813', '180814',\\\n",
    "#           '180815', '180816', '180817', '180818', '180819', '180820', '180821', '180822', '180826']\n",
    "\n",
    "# d_list = ['180827', '180828', '180829', '180830', '180831', '180901', '180902', '180903', '180904', '180905',\\\n",
    "#           '180906', '180907', '180908', '180909', '180910', '180911', '180912', '180913', '180914', '180915',\\\n",
    "#           '180916', '180917', '180918', '180919', '180920', '180921', '180922', '180923', '180924', '180925', \\\n",
    "#           '180926', '180927', '180928', '180929', '180930', '181001', '181002', '181003', '181004', '181005', \\\n",
    "#           '181006', '181007', '181008', '181009', '181010', '181011', '181012', '181013', '181014', '181015', \\\n",
    "#           '181016', '181017', '181018', '181019', '181020', '181021']\n",
    "# d_list = ['181018', '181019']\n",
    "\n",
    "d_list = ['181022', '181023', '181024', '181025', '181026', '181027', '181028', '181029', '181030', \\\n",
    "           '181031', '181101', '181102', '181103', '181104', '181105', '181106', '181107', '181108', '181109',\\\n",
    "           '181110', '181111']\n",
    "\n",
    "day_all = ['180501', '180502', '180503', '180504', '180505', '180506', '180507', '180508', '180509', '180510',\\\n",
    "           '180511', '180512', '180513', '180514', '180515', '180516', '180517', '180518', '180519', '180520',\\\n",
    "           '180521', '180522', '180523', '180524', '180525', '180526', '180527', '180528', '180529', '180530',\\\n",
    "           '180531','180602', '180603', '180604', '180605', '180606', '180607', '180608', '180609', '180610',\\\n",
    "           '180611', '180612', '180613', '180614', '180615', '180616', '180617', '180618', '180619', '180620',\\\n",
    "           '180621', '180622', '180623', '180624', '180625', '180626', '180627', '180628', '180629', '180630',\\\n",
    "           '180701', '180702', '180703', '180704', '180705', '180706', '180707', '180708', '180709', '180710', \\\n",
    "           '180711', '180712', '180713', '180714', '180715',\\\n",
    "           '180716', '180717', '180718', '180719', '180720', '180721', '180722', '180723', '180724', '180725',\\\n",
    "           '180726', '180727', '180728', '180729', '180730', '180731', '180801', '180802', '180803', '180804',\\\n",
    "           '180805', '180806', '180807', '180808', '180809', '180810', '180811', '180812', '180813', '180814',\\\n",
    "           '180815', '180816', '180817', '180818', '180819', '180820', '180821', '180822',  '180826',\\\n",
    "           '180827', '180828', '180829', '180830', '180831', '180901', '180902', '180903', '180904', '180905',\\\n",
    "           '180906', '180907', '180908', '180909', '180910', '180911', '180912', '180913', '180914', '180915',\\\n",
    "           '180916', '180917', '180918', '180919', '180920', '180921', '180922', '180923', '180924', '180925', \\\n",
    "            '180926', '180927', '180928', '180929', '180930', '181001', '181002', '181003', '181004', '181005', \\\n",
    "           '181006', '181007', '181008', '181009', '181010', '181011', '181012', '181013', '181014', '181015', \\\n",
    "           '181016', '181017', '181018', '181019', '181020', '181021',\\\n",
    "           '181022', '181023', '181024', '181025', '181026', '181027', '181028', '181029', '181030', \\\n",
    "           '181031', '181101', '181102', '181103', '181104', '181105', '181106', '181107', '181108', '181109',\\\n",
    "           '181110', '181111']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interval_mint(s):\n",
    "    if s is None:\n",
    "        return 0\n",
    "    value = int(s[0:5])\n",
    "    mytype = s[5:]\n",
    "    if mytype=='M':\n",
    "        value = value\n",
    "    elif mytype=='H':\n",
    "        value = value*60\n",
    "    return value\n",
    "\n",
    "def f_replace_oa(s):\n",
    "    return s.replace('oa','0a')\n",
    "\n",
    "def get_avg_run_stat_mint(s):\n",
    "    # 把时间转换成10进制数据  60进制-->100进制\n",
    "    if s is None:\n",
    "        return None\n",
    "    else:\n",
    "        h = s//10000\n",
    "        m = (s%10000)//100\n",
    "        \n",
    "        s = s%100 #不用精确到秒\n",
    "        return h*60+m+s/60\n",
    "    \n",
    "def my_sum(x):\n",
    "    if x[1] is None or x[0] is None:\n",
    "        return None\n",
    "    else:\n",
    "        return x[1]+x[0]\n",
    "def f_get_jobname_head(s):\n",
    "    return s.split('_')[0]\n",
    "def not_find_end_time(a):\n",
    "    if a==-1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def f_get_hour(x):\n",
    "    t = str(x)[8:10]\n",
    "    return t\n",
    "def get_week_n(day_l):\n",
    "    wk= datetime.datetime.strptime(day_l,'%Y%m%d').weekday()\n",
    "    if wk ==5 or wk ==6:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def get_week(day_l):\n",
    "    return datetime.datetime.strptime(day_l,'%Y%m%d').weekday()\n",
    "f_sum_std = ('max','min','median','mean','std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.ajob表简单处理，加入作业历史完成时间end_time\n",
    "### 2.加入一些统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181022 181023 181024 181025 181026 181027 181028 181029 181030 181031 181101 181102 181103 181104 181105 181106 181107 181108 181109 181110 181111 "
     ]
    }
   ],
   "source": [
    "#ajob_1  \n",
    "#1.对一些变量进行drop\n",
    "#2.对一些日期进行转换\n",
    "#3.获取作业前n天历史end_time ,start_time\n",
    "\n",
    "for day in d_list:\n",
    "    #day = '180602'\n",
    "    day_int = int(day)\n",
    "    day_l = '20'+day\n",
    "    day_4_hour = int('20'+day +'040000')\n",
    "    day_i  = day_all.index(day)\n",
    "    \n",
    "    if day in ('181105', '181106', '181107', '181108', '181109','181110', '181111'):\n",
    "        train_src = pd.read_csv(r'E:\\jupyter\\CTM\\data_src\\ajob\\ajob_%s_0_.txt'%day,low_memory=False)\n",
    "    else:\n",
    "        train_src = pd.read_csv(r'E:\\jupyter\\CTM\\data_src\\ajob\\ajob_%s_0.txt'%day,low_memory=False)\n",
    "\n",
    "    #取4点之前实例化的总数据，排除手工追数情况---------------------------------------------------------------------------\n",
    "    train_src = train_src[train_src['order_time']<=day_4_hour]\n",
    "    train_src = train_src[train_src.odate==day_int]\n",
    "    train_src.drop_duplicates('job_name', keep='last', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    train_src = train_src[train_src.task_type!='SMART Table']  #不预测SMART Table作业完成时间\n",
    "\n",
    "    train_src['from_time_1000_start'] = train_src['from_time'].fillna(0).apply(lambda x:int(x>1000))\n",
    "\n",
    "    train_src.drop('confirm_flag',axis=1,inplace=True)\n",
    "    train_src.drop('job_id',axis=1,inplace=True)\n",
    "    train_src.drop('from_time',axis=1,inplace=True)\n",
    "    train_src.drop('next_time',axis=1,inplace=True)\n",
    "    train_src.drop('to_time',axis=1,inplace=True)\n",
    "    train_src.drop('delete_flag',axis=1,inplace=True)\n",
    "    train_src.drop('rerun_counter',axis=1,inplace=True)\n",
    "    train_src.drop('prev_odate_rerun_counter',axis=1,inplace=True)\n",
    "    train_src.drop('cpu_time',axis=1,inplace=True)\n",
    "    train_src.drop('cpu_id',axis=1,inplace=True)\n",
    "    #train_src.drop('order_d',axis=1,inplace=True)\n",
    "\n",
    "    train_src['nodegroup']= train_src['nodegroup'].fillna('none')\n",
    "    train_src['priority'] = train_src['priority'].apply(f_replace_oa)\n",
    "    #train_src['interval'] = train_src['interval'].apply(get_interval_mint)\n",
    "    train_src['avg_runtime'] = train_src['avg_runtime'].apply(get_avg_run_stat_mint)\n",
    "    train_src['avg_start_time'] = train_src['avg_start_time'].apply(get_avg_run_stat_mint)\n",
    "    train_src['job_name_head'] = train_src['job_name'].apply(f_get_jobname_head)\n",
    "\n",
    "\n",
    "\n",
    "    # 截取数据 4点前数据，做进一步统计---------------------------------------------------------------------------------------\n",
    "\n",
    "    sts_src = train_src[(train_src['end_time']<=day_4_hour)]\n",
    "    sts_src.index = range(len(sts_src))\n",
    "    #print('before 9 data.shap',sts_src.shape)\n",
    "\n",
    "\n",
    "    #待预测数据--------------------------------------------------------------------------------------------------------------\n",
    "    #pre_src = train_src[train_src['end_time']>=end_t_thred]\n",
    "    pre_src = train_src[(train_src['end_time']>=day_4_hour)|(train_src['end_time'].isnull())]\n",
    "    pre_src.index = range(len(pre_src))\n",
    "    #统计特征:\n",
    "\n",
    "    #加入0-4点统计信息---------------------------------------------------------------------------------------------------------\n",
    "    order_job_count = train_src.shape[0]\n",
    "\n",
    "    sts_src['start_hour'] = sts_src['start_time'].apply(f_get_hour)\n",
    "    sts_src['end_hour'] = sts_src['end_time'].apply(f_get_hour)\n",
    "\n",
    "    start_between_0_4_job = sts_src[sts_src.start_time<day_4_hour]\n",
    "    end_between_0_4_job = sts_src[sts_src.end_time<day_4_hour]\n",
    "\n",
    "\n",
    "\n",
    "    start_time_st = start_between_0_4_job.groupby(['start_hour']).start_hour.count()\n",
    "    end_time_st = end_between_0_4_job.groupby(['end_hour']).end_hour.count()\n",
    "\n",
    "    #分组统计 未完成率 % 定义函数--------------------------------------------------------------------------------------------\n",
    "    #按作业名前缀，folder，nodegroup，统计已经实例化作业\n",
    "    orderjob_by_node_group = train_src.groupby(['nodegroup']).nodegroup.count().T.to_dict()\n",
    "    orderjob_by_folder_group = train_src.groupby(['order_table']).order_table.count().T.to_dict()\n",
    "    orderjob_by_job_head_group = train_src.groupby(['job_name_head']).job_name_head.count().T.to_dict()\n",
    "\n",
    "    #按作业名前缀，folder，nodegroup，统计未完成作业完成情况 返回字典\n",
    "    unend_job_by_node_group = pre_src.groupby(['nodegroup']).nodegroup.count().T.to_dict()\n",
    "    unend_job_by_folder_group = pre_src.groupby(['order_table']).order_table.count().T.to_dict()\n",
    "    unend_ob_by_job_head_group = pre_src.groupby(['job_name_head']).job_name_head.count().T.to_dict()\n",
    "\n",
    "    del train_src\n",
    "\n",
    "    def by_nodegroup_group_unfd_rt(nodegroup):\n",
    "        if nodegroup not in unend_job_by_node_group.keys():\n",
    "            return 0\n",
    "        else:\n",
    "            return unend_job_by_node_group[nodegroup]/orderjob_by_node_group[nodegroup]\n",
    "\n",
    "    def by_folder_group_unfd_rt(order_table):\n",
    "        if order_table not in unend_job_by_folder_group.keys():\n",
    "            return 0\n",
    "        else:\n",
    "            return unend_job_by_folder_group[order_table]/orderjob_by_folder_group[order_table]\n",
    "\n",
    "    def by_jobhead_group_unfd_rt(job_name_head):\n",
    "        if job_name_head not in unend_ob_by_job_head_group:\n",
    "            return 0\n",
    "        else: \n",
    "            return unend_ob_by_job_head_group[job_name_head]/orderjob_by_job_head_group[job_name_head]\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #统计全局一样的特征：\n",
    "    train_clms = [cl for cl in pre_src.columns if cl not in ('status','state','interval','max_wait','odate','max_rerun')]\n",
    "\n",
    "    train_gen = pre_src[train_clms]\n",
    "\n",
    "    train_gen['is_weekday'] = get_week_n(day_l)\n",
    "\n",
    "    for i in range(7):\n",
    "        if i == get_week(day_l):\n",
    "            train_gen['weekday_%d'%i] = 1\n",
    "        else:\n",
    "            train_gen['weekday_%d'%i] = 0\n",
    "\n",
    "\n",
    "    train_gen['td_ord__count'] = order_job_count\n",
    "\n",
    "    #train_gen['odate'] = train_gen['odate'].apply(lambda x:x==int(day)).astype(int)\n",
    "\n",
    "    train_gen['start_time_00_rate'] = (start_time_st/order_job_count)[0]\n",
    "    train_gen['start_time_01_rate'] = (start_time_st/order_job_count)[1]\n",
    "    train_gen['start_time_02_rate'] = (start_time_st/order_job_count)[2]\n",
    "    train_gen['start_time_03_rate'] = (start_time_st/order_job_count)[3]\n",
    "    train_gen['start_time_03sum_rate'] = (start_time_st.cumsum()/order_job_count)[3]\n",
    "    #train_gen['unfinsih_03sum_rt'] = (start_time_st.cumsum()/order_job_count-end_time_st.cumsum()/order_job_count)[3]\n",
    "\n",
    "\n",
    "    #截取时间前未完成作业分组 率：\n",
    "\n",
    "    train_gen['by_nodegroup_unfd_rt_pred'] = train_gen['nodegroup'].apply(by_nodegroup_group_unfd_rt)\n",
    "    train_gen['by_folder_unfd_rt_pred'] = train_gen['order_table'].apply(by_folder_group_unfd_rt)\n",
    "    train_gen['by_jobhead_unfd_rt_pred'] = train_gen['job_name_head'].apply(by_jobhead_group_unfd_rt)\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #获取前 n天历史结束时间---函数-----------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_job_n_day_before_end_time(n,job_list):\n",
    "        day = day_all[day_i - n]\n",
    "        #print(day)\n",
    "        day_begain = int('20'+day+'000000')\n",
    "        day_4 = int('20'+day+'040000') #实例化要在4点前，排除手工追数情况;\n",
    "        day_int = int(day)\n",
    "\n",
    "        if n ==0:\n",
    "            df =train_gen\n",
    "            #print(day)\n",
    "        else:\n",
    "            df = pd.read_csv(r'E:\\jupyter\\CTM\\data_src\\ajob\\ajob_%s_0.txt'%day,low_memory=False)\n",
    "            df = df[df.odate==day_int]\n",
    "            df = df[df['order_time']<day_4]\n",
    "\n",
    "        df = df[df.end_time.notnull()]\n",
    "        #print(df.shape)\n",
    "\n",
    "\n",
    "        #排除空值，手工追数非自动跑批数据.\n",
    "\n",
    "\n",
    "        dat_start = datetime.datetime.strptime(str(day_begain),'%Y%m%d%H%M%S')\n",
    "\n",
    "        def f_get_mint_bef(x):\n",
    "            dat_t = datetime.datetime.strptime(str(x),'%Y%m%d%H%M%S')\n",
    "            dt = (dat_t-dat_start).total_seconds()//60\n",
    "            return dt\n",
    "\n",
    "        df['end_time'] = df['end_time'].apply(int).apply(f_get_mint_bef)\n",
    "        df['start_time'] = df['start_time'].apply(int).apply(f_get_mint_bef)\n",
    "\n",
    "        df_start = df[['job_name','start_time']]\n",
    "        df_end = df[['job_name','end_time']]\n",
    "\n",
    "        df_start_dict = df_start.groupby('job_name').start_time.max().T.to_dict()\n",
    "        df_end_dict = df_end.groupby('job_name').end_time.max().T.to_dict()\n",
    "\n",
    "        start_not_findjob = list(set(job_list)-set(df_start_dict.keys()))\n",
    "        end_not_findjob = list(set(job_list)-set(df_end_dict.keys()))\n",
    "\n",
    "        for job in start_not_findjob:\n",
    "            df_start_dict[job] = -1\n",
    "        for job in end_not_findjob:\n",
    "            df_end_dict[job] = -1\n",
    "\n",
    "        return (job_list.apply(lambda x:df_start_dict[x]),job_list.apply(lambda x:df_end_dict[x]))\n",
    "\n",
    "    #统计前 n  天日期 完成时间 ----------------------------------------------------------------------------------------\n",
    "    day_before_list = [0]+ list(range(7,21))\n",
    "\n",
    "    for k in day_before_list:\n",
    "        #print(k)\n",
    "        train_gen['day_%d_bef_start_time'%k],train_gen['day_%d_bef_end_time'%k] = get_job_n_day_before_end_time(k,train_gen['job_name'])\n",
    "\n",
    "    day_1_7_bef_start_time = ['day_%d_bef_start_time'%k for k in range(7,15)]\n",
    "    day_1_14_bef_start_time = ['day_%d_bef_start_time'%k for k in range(7,21)]\n",
    "\n",
    "    day_1_7_bef_end_time = ['day_%d_bef_end_time'%k for k in range(7,15)]\n",
    "    day_1_14_bef_end_time = ['day_%d_bef_end_time'%k for k in range(7,21)]\n",
    "\n",
    "\n",
    "    #填充缺失值\n",
    "    for k in range(1,15):\n",
    "        train_gen[day_1_14_bef_start_time].fillna(method=\"ffill\",axis=1,inplace=True)\n",
    "        train_gen[day_1_14_bef_end_time].fillna(method=\"ffill\",axis=1,inplace=True)\n",
    "\n",
    "        train_gen[day_1_14_bef_start_time].fillna(method=\"bfill\",axis=1,inplace=True)\n",
    "        train_gen[day_1_14_bef_end_time].fillna(method=\"bfill\",axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    for f in f_sum_std:\n",
    "        train_gen['day_1_7_bef_start_time_%s'%f] = train_gen[day_1_7_bef_start_time].apply(f, axis=1)\n",
    "        train_gen['day_1_14_bef_start_time_%s'%f] = train_gen[day_1_14_bef_start_time].apply(f, axis=1)\n",
    "\n",
    "        train_gen['day_1_7_bef_end_time_%s'%f] = train_gen[day_1_7_bef_end_time].apply(f, axis=1)\n",
    "        train_gen['day_1_14_bef_end_time_%s'%f] = train_gen[day_1_14_bef_end_time].apply(f, axis=1)\n",
    "\n",
    "    start_time_list = ['avg_start_time','day_1_7_bef_start_time_mean','day_1_7_bef_start_time_median',\\\n",
    "                       'day_1_14_bef_start_time_mean','day_1_14_bef_start_time_median']\n",
    "\n",
    "\n",
    "    for i in range(15,21):\n",
    "        train_gen.drop('day_%d_bef_start_time'%i,axis=1,inplace=True)\n",
    "        train_gen.drop('day_%d_bef_end_time'%i,axis=1,inplace=True)\n",
    "\n",
    "    train_gen['day_7_bef_no_end_time'] =train_gen['day_1_7_bef_end_time_min'].apply(not_find_end_time)\n",
    "    train_gen['day_14_bef_no_end_time'] =train_gen['day_1_14_bef_end_time_min'].apply(not_find_end_time)\n",
    "    #统计历史end_time 结束----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    del train_gen['order_time']\n",
    "    train_gen = train_gen.round(3)\n",
    "\n",
    "    #order_id  加入server信息，保证唯一ID 和其他表关联    \n",
    "    train_gen['v_date'] = train_gen['v_date'].apply(str)\n",
    "    train_gen['order_id'] = train_gen[['order_id','v_date']].apply(lambda x:x[0]+x[1][7:9],axis=1) \n",
    "    train_gen['v_date'] = train_gen['v_date'].apply(lambda x:x[:6])\n",
    "\n",
    "    train_gen['avg_start_error'] = train_gen['avg_start_time'].apply(lambda x:int(x>4000))\n",
    "\n",
    "    train_gen.to_csv(r'E:\\jupyter\\CTM\\data_processed\\ajob\\ajob_%s_1.csv'%(day),index=False)\n",
    "\n",
    "    print(day,end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
