{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtr = xgb.XGBRegressor(max_depth=7, learning_rate=0.1)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "train_list = [ '180609', '180610',\\\n",
    "        '180611', '180612', '180613', '180614', '180615', '180616', '180617', '180618', '180619', '180620',\\\n",
    "        '180621', '180622', '180623', '180624', '180625', '180626', '180627', '180628', '180629', '180630',\\\n",
    "        '180701', '180702', '180703', '180704', '180705', '180706']\n",
    "\n",
    "valid_list = ['180707', '180708']\n",
    "\n",
    "test_list = ['180709', '180710', '180711', '180712', '180713', '180714', '180715']\n",
    "\n",
    "day_all = ['171211', '171212', '171213', '171214', '171215', '171216', '171217', '171218', '171219', '171220', \\\n",
    "           '171221', '171222', '171223', '171224', '171225', '171226', '171227', '171228', '171229', '171230', \\\n",
    "           '171231', '180101', '180102', '180104', '180105', '180106', '180107', '180108', '180109', '180110', \\\n",
    "           '180111', '180112', '180113', '180114', '180115', '180116', '180117', '180118', '180119', '180120', \\\n",
    "           '180121', '180122', '180123', '180124', '180125', '180126', '180127', '180128', '180129', '180130', '180131',\\\n",
    "           '180201', '180202', '180203', '180204', '180205', '180206', '180207', '180208', '180209', '180210',\\\n",
    "           '180211', '180212', '180213', '180214', '180215', '180216', '180217', '180218', '180219', '180220', \\\n",
    "           '180221', '180222', '180223', '180224', '180225', '180226', '180227', '180228', \\\n",
    "           '180301', '180302', '180303', '180304', '180305', '180306', '180307', '180308', '180309', '180310', \\\n",
    "           '180311', '180312', '180313', '180314', '180315', '180316', '180317', '180318', '180319', '180320',\\\n",
    "           '180321', '180322', '180323', '180324', '180325', '180326', '180327', '180328', '180329', '180330', '180331', \\\n",
    "           '180401', '180402', '180403', '180404', '180405', '180406', '180407', '180408', '180409', '180410', \\\n",
    "           '180411', '180412', '180413', '180414', '180415', '180416', '180417','180418', '180419', '180420',\\\n",
    "           '180421', '180422', '180423', '180424', '180425', '180426', '180427', '180428', '180429', '180430',\\\n",
    "           '180501', '180502', '180503', '180504', '180505', '180506', '180507', '180508', '180509', '180510',\\\n",
    "           '180511', '180512', '180513', '180514', '180515', '180516', '180517', '180518', '180519', '180520',\\\n",
    "           '180521', '180522', '180523', '180524', '180525', '180526', '180527', '180528', '180529', '180530',\\\n",
    "           '180531','180602', '180603', '180604', '180605', '180606', '180607', '180608', '180609', '180610',\\\n",
    "           '180611', '180612', '180613', '180614', '180615', '180616', '180617', '180618', '180619', '180620',\\\n",
    "           '180621', '180622', '180623', '180624', '180625', '180626', '180627', '180628', '180629', '180630',\\\n",
    "           '180701', '180702', '180703', '180704', '180705', '180706', '180707', '180708', '180709', '180710', \\\n",
    "           '180711', '180712', '180713', '180714', '180715','180716','180717']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180609 180610 180611 180612 180613 180614 180615 180616 180617 180618 180619 180620 180621 180622 180623 180624 180625 180626 180627 180628 180629 180630 180701 180702 180703 180704 180705 180706 180707 180708 "
     ]
    }
   ],
   "source": [
    "#读取数据 按天\n",
    "\n",
    "# 划分训练集， 验证集 测试集\n",
    "\n",
    "train_df = []\n",
    "for day in train_list:\n",
    "    print(day,end=' ')   \n",
    "    day_all_data = pd.read_csv(r'E:\\jupyter\\CTM\\valid_data\\train_dt\\with_res_train_%s_dt.csv'%day)\n",
    "    \n",
    "    #day_all_data = day_all_data[day_all_data.end_time.notnull()]\n",
    "    \n",
    "    #过滤结束日期超过2天的\n",
    "    day_after_2 = day_all[day_all.index(day)+2]\n",
    "    day_after_2_int = int('20'+day_after_2+'000000')\n",
    "    day_all_data = day_all_data[day_all_data.end_time<day_after_2_int]\n",
    "    \n",
    "    train_df.append(day_all_data.sample(frac=0.1))\n",
    "    \n",
    "train = pd.concat(train_df, ignore_index=True)\n",
    "\n",
    "valid_df = []\n",
    "for day in valid_list:\n",
    "    print(day,end=' ')   \n",
    "    day_all_data = pd.read_csv(r'E:\\jupyter\\CTM\\valid_data\\train_dt\\with_res_train_%s_dt.csv'%day)\n",
    "    #day_all_data = day_all_data[day_all_data.end_time.notnull()]\n",
    "    day_after_2 = day_all[day_all.index(day)+2]\n",
    "    day_after_2_int = int('20'+day_after_2+'000000')\n",
    "    day_all_data = day_all_data[day_all_data.end_time<day_after_2_int]\n",
    "    \n",
    "    valid_df.append(day_all_data)\n",
    "    \n",
    "valid = pd.concat(valid_df, ignore_index=True)\n",
    "\n",
    "# test_df = []\n",
    "\n",
    "# for day in test_list:\n",
    "#     print(day,end=' ')   \n",
    "#     day_all_data = pd.read_csv(r'E:\\jupyter\\CTM\\valid_data\\train/train_%s_edt.csv'%day)\n",
    "#     day_all_data = day_all_data[day_all_data.end_time.notnull()]\n",
    "#     test_df.append(day_all_data)\n",
    "    \n",
    "# test = pd.concat(test_df, ignore_index=True)\n",
    "\n",
    "#空值处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382005, 226)\n",
      "(282561, 225)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols =['day_bef_1_dt','day_bef_3_dt','day_bef_2_dt']\n",
    "cols = ['day_bef_%d_dt'%i for i in range(1,8)]\n",
    "\n",
    "train[cols].fillna(method=\"ffill\",axis=1,inplace=True)\n",
    "train[cols].fillna(method=\"bfill\",axis=1,inplace=True)\n",
    "\n",
    "valid[cols].fillna(method=\"ffill\",axis=1,inplace=True)\n",
    "valid[cols].fillna(method=\"bfill\",axis=1,inplace=True)\n",
    "\n",
    "\n",
    "train['dt_std'] = train[cols].apply('std',axis=1)\n",
    "valid['dt_std'] = valid[cols].apply('std',axis=1)\n",
    "\n",
    "train['dt_mean'] = train[cols].apply('mean',axis=1)\n",
    "valid['dt_mean'] = valid[cols].apply('mean',axis=1)\n",
    "\n",
    "train['dt_median'] = train[cols].apply('median',axis=1)\n",
    "valid['dt_median'] = valid[cols].apply('median',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(305604, 218)\n",
      "(76401, 218)\n",
      "(282561, 218)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#过滤训练不需要的数据字段\n",
    "cols_train =[clm for clm in valid.columns if clm not in ['order_id','job_name','v_date','order_d','from_time_1000_start',\\\n",
    "                                                            'start_time','end_time','order_id','job_name','day_0_bef_start_time',\\\n",
    "                                                            'day_0_bef_end_time','day_bef_0_dt']]\n",
    "\n",
    "\n",
    "valid_x = valid[cols_train]\n",
    "valid_y = valid['day_bef_0_dt']\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(train[cols_train],train['day_bef_0_dt'],random_state=33,test_size=0.2)\n",
    "\n",
    "train_y = train_y.fillna(train_y.mean())\n",
    "valid_y = valid_y.fillna(valid_y.mean())\n",
    "test_y = test_y.fillna(test_y.mean())  \n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(valid_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['avg_runtime', 'cyclic', 'avg_start_time', 'is_weekday', 'weekday_0',\n",
       "       'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5',\n",
       "       ...\n",
       "       'GRP_OFB_AGENT', 'GRP_TD_AGENT', 'othernode', '1', '2', '3',\n",
       "       'day_bef_1_dt', 'day_bef_2_dt', 'day_bef_3_dt', 'dt_std'],\n",
       "      dtype='object', length=210)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:\n",
      "42.0084308029\n",
      "\n",
      "train:\n",
      "27.3000429866\n"
     ]
    }
   ],
   "source": [
    "# Xgboost算法预测\n",
    "dtr = xgb.XGBRegressor(max_depth=7, learning_rate=0.1)\n",
    "\n",
    "dtr.fit(train_x,train_y)\n",
    "\n",
    "valid_predict = dtr.predict(valid_x)\n",
    "\n",
    "test_predict = dtr.predict(test_x)\n",
    "\n",
    "print('valid:')\n",
    "print(mean_absolute_error(valid_y,valid_predict))\n",
    "#print(mean_squared_error(valid_y,valid_predict))\n",
    "\n",
    "print()\n",
    "print('train:')\n",
    "print(mean_absolute_error(test_y,test_predict))\n",
    "#print(mean_squared_error(test_y,test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:\n",
      "42.1272706724\n",
      "\n",
      "train:\n",
      "29.0138922384\n"
     ]
    }
   ],
   "source": [
    "# Xgboost算法预测\n",
    "dtr = xgb.XGBRegressor(max_depth=6, learning_rate=0.1)\n",
    "\n",
    "dtr.fit(train_x,train_y)\n",
    "\n",
    "valid_predict = dtr.predict(valid_x)\n",
    "\n",
    "test_predict = dtr.predict(test_x)\n",
    "\n",
    "print('valid:')\n",
    "print(mean_absolute_error(valid_y,valid_predict))\n",
    "#print(mean_squared_error(valid_y,valid_predict))\n",
    "\n",
    "print()\n",
    "print('train:')\n",
    "print(mean_absolute_error(test_y,test_predict))\n",
    "#print(mean_squared_error(test_y,test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid['pred_dt'] = valid_predict\n",
    "valid[valid.v_date==180707].to_csv('vlid_pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imports = pd.read_csv(r'feature_score_20181011.csv')['feature'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95        day_1_7_bef_end_time_std\n",
       "96      day_1_14_bef_end_time_mean\n",
       "97                           new_4\n",
       "98    day_1_14_bef_start_time_mean\n",
       "99       day_1_14_bef_end_time_max\n",
       "Name: feature, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imports.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tcv_agg's rmse: 81.0965 + 0.920876\n",
      "[100]\tcv_agg's rmse: 77.1452 + 0.921392\n",
      "best n_estimators: 100\n",
      "best cv score: 77.1451615276\n"
     ]
    }
   ],
   "source": [
    "# lightgbm 算法预测\n",
    "import lightgbm as lgb\n",
    "\n",
    "params = {    'boosting_type': 'gbdt', \n",
    "    'objective': 'regression', \n",
    "    'learning_rate': 0.1, \n",
    "    'num_leaves': 50, \n",
    "    'max_depth': 6,    'subsample': 0.8, \n",
    "    'colsample_bytree': 0.8, \n",
    "    }\n",
    "\n",
    "data_train = lgb.Dataset(train_x[feature_imports].values, train_y.values, silent=True)\n",
    "cv_results = lgb.cv(\n",
    "    params, data_train, num_boost_round=100, nfold=5, stratified=False, shuffle=True, metrics='rmse',\n",
    "    early_stopping_rounds=10, verbose_eval=50, show_stdv=True, seed=0)\n",
    "\n",
    "print('best n_estimators:', len(cv_results['rmse-mean']))\n",
    "print('best cv score:', cv_results['rmse-mean'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -9203.62820, std: 262.94918, params: {'max_depth': 3, 'num_leaves': 50}, mean: -9203.62820, std: 262.94918, params: {'max_depth': 3, 'num_leaves': 80}, mean: -9203.62820, std: 262.94918, params: {'max_depth': 3, 'num_leaves': 110}, mean: -9203.62820, std: 262.94918, params: {'max_depth': 3, 'num_leaves': 140}, mean: -7315.00261, std: 270.81834, params: {'max_depth': 5, 'num_leaves': 50}, mean: -7315.00261, std: 270.81834, params: {'max_depth': 5, 'num_leaves': 80}, mean: -7315.00261, std: 270.81834, params: {'max_depth': 5, 'num_leaves': 110}, mean: -7315.00261, std: 270.81834, params: {'max_depth': 5, 'num_leaves': 140}, mean: -6510.25748, std: 247.02227, params: {'max_depth': 7, 'num_leaves': 50}, mean: -6186.02312, std: 277.57907, params: {'max_depth': 7, 'num_leaves': 80}, mean: -6117.11472, std: 257.17521, params: {'max_depth': 7, 'num_leaves': 110}, mean: -6107.59998, std: 272.80642, params: {'max_depth': 7, 'num_leaves': 140}] {'max_depth': 7, 'num_leaves': 140} -6107.5999848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "### 我们可以创建lgb的sklearn模型，使用上面选择的(学习率，评估器数目)\n",
    "df_train = train_x[feature_imports].values\n",
    "y_train = train_y.values\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=43, max_depth=6,\n",
    "                              metric='rmse', bagging_fraction = 0.8,feature_fraction = 0.8)\n",
    "\n",
    "params_test1={'max_depth': range(3,8,2),'num_leaves':range(50, 170, 30)\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator=model_lgb, param_grid=params_test1, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "\n",
    "\n",
    "gsearch1.fit(df_train, y_train)\n",
    "print(gsearch1.grid_scores_)\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -6673.27758, std: 273.84875, params: {'max_depth': 6, 'num_leaves': 110}, mean: -6673.27758, std: 273.84875, params: {'max_depth': 6, 'num_leaves': 120}, mean: -6673.27758, std: 273.84875, params: {'max_depth': 6, 'num_leaves': 130}, mean: -6673.27758, std: 273.84875, params: {'max_depth': 6, 'num_leaves': 140}, mean: -6673.27758, std: 273.84875, params: {'max_depth': 6, 'num_leaves': 150}, mean: -6133.27428, std: 256.60345, params: {'max_depth': 7, 'num_leaves': 110}, mean: -6108.17985, std: 259.06689, params: {'max_depth': 7, 'num_leaves': 120}, mean: -6108.17985, std: 259.06689, params: {'max_depth': 7, 'num_leaves': 130}, mean: -6108.17985, std: 259.06689, params: {'max_depth': 7, 'num_leaves': 140}, mean: -6108.17985, std: 259.06689, params: {'max_depth': 7, 'num_leaves': 150}, mean: -5827.51194, std: 276.06865, params: {'max_depth': 8, 'num_leaves': 110}, mean: -5760.27608, std: 266.33728, params: {'max_depth': 8, 'num_leaves': 120}, mean: -5733.11528, std: 256.66268, params: {'max_depth': 8, 'num_leaves': 130}, mean: -5721.71388, std: 273.42888, params: {'max_depth': 8, 'num_leaves': 140}, mean: -5686.24328, std: 259.75197, params: {'max_depth': 8, 'num_leaves': 150}]\n",
      "{'max_depth': 8, 'num_leaves': 150}\n",
      "-5686.24327531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=43, max_depth=6,\n",
    "                              metric='rmse', bagging_fraction = 0.9,feature_fraction = 0.9)\n",
    "\n",
    "params_test1={'max_depth': [6,7,8],'num_leaves':[110,120,130,140,150]\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator=model_lgb, param_grid=params_test1, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "\n",
    "\n",
    "gsearch1.fit(df_train, y_train)\n",
    "print(gsearch1.grid_scores_)\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:  4.8min finished\n",
      "c:\\program files\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -5659.97168, std: 286.94259, params: {'min_child_samples': 18, 'min_child_weight': 0.001},\n",
       "  mean: -5659.97168, std: 286.94259, params: {'min_child_samples': 18, 'min_child_weight': 0.002},\n",
       "  mean: -5686.54144, std: 271.40079, params: {'min_child_samples': 19, 'min_child_weight': 0.001},\n",
       "  mean: -5686.54144, std: 271.40079, params: {'min_child_samples': 19, 'min_child_weight': 0.002},\n",
       "  mean: -5686.24328, std: 259.75197, params: {'min_child_samples': 20, 'min_child_weight': 0.001},\n",
       "  mean: -5686.24328, std: 259.75197, params: {'min_child_samples': 20, 'min_child_weight': 0.002},\n",
       "  mean: -5690.79231, std: 276.92992, params: {'min_child_samples': 21, 'min_child_weight': 0.001},\n",
       "  mean: -5690.79231, std: 276.92992, params: {'min_child_samples': 21, 'min_child_weight': 0.002},\n",
       "  mean: -5704.46607, std: 263.36543, params: {'min_child_samples': 22, 'min_child_weight': 0.001},\n",
       "  mean: -5704.46607, std: 263.36543, params: {'min_child_samples': 22, 'min_child_weight': 0.002}],\n",
       " {'min_child_samples': 18, 'min_child_weight': 0.001},\n",
       " -5659.9716764723289)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_test3={    'min_child_samples': [18, 19, 20, 21, 22],    'min_child_weight':[0.001, 0.002]\n",
    "}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=150,\n",
    "                              learning_rate=0.1, n_estimators=43, max_depth=8, \n",
    "                              metric='rmse', bagging_fraction = 0.9, feature_fraction = 0.9)\n",
    "gsearch3 = GridSearchCV(estimator=model_lgb, param_grid=params_test3, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch3.fit(df_train, y_train)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done 150 out of 150 | elapsed: 12.0min finished\n",
      "c:\\program files\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -5969.29090, std: 254.05129, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.5},\n",
       "  mean: -5940.67199, std: 276.86007, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.6},\n",
       "  mean: -5931.52248, std: 228.38089, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.7},\n",
       "  mean: -5924.73321, std: 267.25357, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.8},\n",
       "  mean: -5933.10128, std: 270.46462, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.9},\n",
       "  mean: -5926.28559, std: 276.91346, params: {'bagging_fraction': 0.6, 'feature_fraction': 1},\n",
       "  mean: -5900.75024, std: 262.40613, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.5},\n",
       "  mean: -5822.16782, std: 243.72340, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.6},\n",
       "  mean: -5832.76708, std: 266.38897, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.7},\n",
       "  mean: -5856.07236, std: 274.33782, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.8},\n",
       "  mean: -5836.87832, std: 295.13380, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.9},\n",
       "  mean: -5826.55185, std: 270.84369, params: {'bagging_fraction': 0.7, 'feature_fraction': 1},\n",
       "  mean: -5806.12630, std: 238.65915, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.5},\n",
       "  mean: -5753.25122, std: 249.62818, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.6},\n",
       "  mean: -5791.01924, std: 259.07336, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.7},\n",
       "  mean: -5781.54779, std: 235.81367, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.8},\n",
       "  mean: -5756.93478, std: 254.81780, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.9},\n",
       "  mean: -5771.10528, std: 273.05238, params: {'bagging_fraction': 0.8, 'feature_fraction': 1},\n",
       "  mean: -5754.63954, std: 228.22064, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.5},\n",
       "  mean: -5721.97425, std: 251.02727, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.6},\n",
       "  mean: -5731.35355, std: 245.62903, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.7},\n",
       "  mean: -5748.83394, std: 252.26849, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.8},\n",
       "  mean: -5760.61599, std: 250.03199, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.9},\n",
       "  mean: -5774.66919, std: 272.03400, params: {'bagging_fraction': 0.9, 'feature_fraction': 1},\n",
       "  mean: -5700.39018, std: 247.12463, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.5},\n",
       "  mean: -5718.69526, std: 270.60925, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.6},\n",
       "  mean: -5681.33469, std: 245.77983, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.7},\n",
       "  mean: -5688.97353, std: 279.34301, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.8},\n",
       "  mean: -5659.97168, std: 286.94259, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.9},\n",
       "  mean: -5665.25136, std: 270.78269, params: {'bagging_fraction': 1.0, 'feature_fraction': 1}],\n",
       " {'bagging_fraction': 1.0, 'feature_fraction': 0.9},\n",
       " -5659.9716764723289)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_test4={    'feature_fraction': [0.5, 0.6, 0.7, 0.8, 0.9,1],    'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=150,\n",
    "                              learning_rate=0.1, n_estimators=43, max_depth=8, \n",
    "                              metric='rmse', bagging_freq = 5,  min_child_samples=18)\n",
    "gsearch4 = GridSearchCV(estimator=model_lgb, param_grid=params_test4, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch4.fit(df_train, y_train)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 49 candidates, totalling 245 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=4)]: Done 245 out of 245 | elapsed: 19.0min finished\n",
      "c:\\program files\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -6099.58834, std: 269.13829, params: {'reg_alpha': 0, 'reg_lambda': 0},\n",
       "  mean: -6097.79398, std: 269.30194, params: {'reg_alpha': 0, 'reg_lambda': 0.001},\n",
       "  mean: -6094.15970, std: 251.67945, params: {'reg_alpha': 0, 'reg_lambda': 0.01},\n",
       "  mean: -6105.40444, std: 262.07572, params: {'reg_alpha': 0, 'reg_lambda': 0.03},\n",
       "  mean: -6075.33383, std: 264.19015, params: {'reg_alpha': 0, 'reg_lambda': 0.08},\n",
       "  mean: -6093.35649, std: 285.44308, params: {'reg_alpha': 0, 'reg_lambda': 0.3},\n",
       "  mean: -6098.34910, std: 283.63691, params: {'reg_alpha': 0, 'reg_lambda': 0.5},\n",
       "  mean: -6099.58842, std: 269.13828, params: {'reg_alpha': 0.001, 'reg_lambda': 0},\n",
       "  mean: -6097.80654, std: 269.31024, params: {'reg_alpha': 0.001, 'reg_lambda': 0.001},\n",
       "  mean: -6094.15978, std: 251.67945, params: {'reg_alpha': 0.001, 'reg_lambda': 0.01},\n",
       "  mean: -6105.37624, std: 262.05878, params: {'reg_alpha': 0.001, 'reg_lambda': 0.03},\n",
       "  mean: -6075.33391, std: 264.19015, params: {'reg_alpha': 0.001, 'reg_lambda': 0.08},\n",
       "  mean: -6093.35656, std: 285.44307, params: {'reg_alpha': 0.001, 'reg_lambda': 0.3},\n",
       "  mean: -6098.34676, std: 283.63841, params: {'reg_alpha': 0.001, 'reg_lambda': 0.5},\n",
       "  mean: -6099.58790, std: 269.13912, params: {'reg_alpha': 0.01, 'reg_lambda': 0},\n",
       "  mean: -6097.80722, std: 269.31021, params: {'reg_alpha': 0.01, 'reg_lambda': 0.001},\n",
       "  mean: -6094.16048, std: 251.67941, params: {'reg_alpha': 0.01, 'reg_lambda': 0.01},\n",
       "  mean: -6105.37691, std: 262.05873, params: {'reg_alpha': 0.01, 'reg_lambda': 0.03},\n",
       "  mean: -6075.33459, std: 264.19011, params: {'reg_alpha': 0.01, 'reg_lambda': 0.08},\n",
       "  mean: -6093.35722, std: 285.44302, params: {'reg_alpha': 0.01, 'reg_lambda': 0.3},\n",
       "  mean: -6098.34742, std: 283.63836, params: {'reg_alpha': 0.01, 'reg_lambda': 0.5},\n",
       "  mean: -6109.42977, std: 266.72485, params: {'reg_alpha': 0.03, 'reg_lambda': 0},\n",
       "  mean: -6104.84027, std: 265.22708, params: {'reg_alpha': 0.03, 'reg_lambda': 0.001},\n",
       "  mean: -6094.16203, std: 251.67934, params: {'reg_alpha': 0.03, 'reg_lambda': 0.01},\n",
       "  mean: -6105.40664, std: 262.07557, params: {'reg_alpha': 0.03, 'reg_lambda': 0.03},\n",
       "  mean: -6071.11270, std: 262.05158, params: {'reg_alpha': 0.03, 'reg_lambda': 0.08},\n",
       "  mean: -6093.35868, std: 285.44290, params: {'reg_alpha': 0.03, 'reg_lambda': 0.3},\n",
       "  mean: -6098.34889, std: 283.63826, params: {'reg_alpha': 0.03, 'reg_lambda': 0.5},\n",
       "  mean: -6109.43351, std: 266.72469, params: {'reg_alpha': 0.08, 'reg_lambda': 0},\n",
       "  mean: -6104.84186, std: 265.22338, params: {'reg_alpha': 0.08, 'reg_lambda': 0.001},\n",
       "  mean: -6094.16591, std: 251.67915, params: {'reg_alpha': 0.08, 'reg_lambda': 0.01},\n",
       "  mean: -6105.39152, std: 262.06406, params: {'reg_alpha': 0.08, 'reg_lambda': 0.03},\n",
       "  mean: -6071.11546, std: 262.05203, params: {'reg_alpha': 0.08, 'reg_lambda': 0.08},\n",
       "  mean: -6097.96799, std: 282.36094, params: {'reg_alpha': 0.08, 'reg_lambda': 0.3},\n",
       "  mean: -6083.17421, std: 256.39389, params: {'reg_alpha': 0.08, 'reg_lambda': 0.5},\n",
       "  mean: -6108.24167, std: 262.49639, params: {'reg_alpha': 0.3, 'reg_lambda': 0},\n",
       "  mean: -6093.70872, std: 272.60797, params: {'reg_alpha': 0.3, 'reg_lambda': 0.001},\n",
       "  mean: -6093.63266, std: 251.31606, params: {'reg_alpha': 0.3, 'reg_lambda': 0.01},\n",
       "  mean: -6107.10188, std: 260.53487, params: {'reg_alpha': 0.3, 'reg_lambda': 0.03},\n",
       "  mean: -6073.45888, std: 259.99326, params: {'reg_alpha': 0.3, 'reg_lambda': 0.08},\n",
       "  mean: -6100.60669, std: 283.67023, params: {'reg_alpha': 0.3, 'reg_lambda': 0.3},\n",
       "  mean: -6083.19069, std: 256.39307, params: {'reg_alpha': 0.3, 'reg_lambda': 0.5},\n",
       "  mean: -6094.50308, std: 259.63718, params: {'reg_alpha': 0.5, 'reg_lambda': 0},\n",
       "  mean: -6087.76068, std: 266.24763, params: {'reg_alpha': 0.5, 'reg_lambda': 0.001},\n",
       "  mean: -6091.71275, std: 253.05596, params: {'reg_alpha': 0.5, 'reg_lambda': 0.01},\n",
       "  mean: -6090.37138, std: 277.43268, params: {'reg_alpha': 0.5, 'reg_lambda': 0.03},\n",
       "  mean: -6079.47090, std: 266.99308, params: {'reg_alpha': 0.5, 'reg_lambda': 0.08},\n",
       "  mean: -6096.40632, std: 288.07255, params: {'reg_alpha': 0.5, 'reg_lambda': 0.3},\n",
       "  mean: -6098.97952, std: 283.20323, params: {'reg_alpha': 0.5, 'reg_lambda': 0.5}],\n",
       " {'reg_alpha': 0.03, 'reg_lambda': 0.08},\n",
       " -6071.1126986921981)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_test6={    'reg_alpha': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],    'reg_lambda': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5]\n",
    "}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=120,\n",
    "                              learning_rate=0.1, n_estimators=43, max_depth=7, \n",
    "                              metric='rmse',  min_child_samples=18, feature_fraction=0.9)\n",
    "gsearch6 = GridSearchCV(estimator=model_lgb, param_grid=params_test6, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch6.fit(df_train, y_train)\n",
    "gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\tvalid_0's l2: 69950.5\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's l2: 58806.9\n",
      "[3]\tvalid_0's l2: 49743.8\n",
      "[4]\tvalid_0's l2: 42348.6\n",
      "[5]\tvalid_0's l2: 36356.8\n",
      "[6]\tvalid_0's l2: 31446\n",
      "[7]\tvalid_0's l2: 27445\n",
      "[8]\tvalid_0's l2: 24220.2\n",
      "[9]\tvalid_0's l2: 21530.2\n",
      "[10]\tvalid_0's l2: 19333.3\n",
      "[11]\tvalid_0's l2: 17520.9\n",
      "[12]\tvalid_0's l2: 16014.3\n",
      "[13]\tvalid_0's l2: 14785.3\n",
      "[14]\tvalid_0's l2: 13731.5\n",
      "[15]\tvalid_0's l2: 12900.1\n",
      "[16]\tvalid_0's l2: 12199.9\n",
      "[17]\tvalid_0's l2: 11631.7\n",
      "[18]\tvalid_0's l2: 11157.1\n",
      "[19]\tvalid_0's l2: 10732.3\n",
      "[20]\tvalid_0's l2: 10368.9\n",
      "[21]\tvalid_0's l2: 10085.9\n",
      "[22]\tvalid_0's l2: 9846.7\n",
      "[23]\tvalid_0's l2: 9642.86\n",
      "[24]\tvalid_0's l2: 9466.02\n",
      "[25]\tvalid_0's l2: 9280.34\n",
      "[26]\tvalid_0's l2: 9145.08\n",
      "[27]\tvalid_0's l2: 9004.67\n",
      "[28]\tvalid_0's l2: 8891.16\n",
      "[29]\tvalid_0's l2: 8794.45\n",
      "[30]\tvalid_0's l2: 8682.58\n",
      "[31]\tvalid_0's l2: 8598.62\n",
      "[32]\tvalid_0's l2: 8544.47\n",
      "[33]\tvalid_0's l2: 8458.91\n",
      "[34]\tvalid_0's l2: 8398.91\n",
      "[35]\tvalid_0's l2: 8301.78\n",
      "[36]\tvalid_0's l2: 8219.6\n",
      "[37]\tvalid_0's l2: 8172.73\n",
      "[38]\tvalid_0's l2: 8109.17\n",
      "[39]\tvalid_0's l2: 8038.38\n",
      "[40]\tvalid_0's l2: 7998.38\n",
      "[41]\tvalid_0's l2: 7958.6\n",
      "[42]\tvalid_0's l2: 7930.4\n",
      "[43]\tvalid_0's l2: 7895.48\n",
      "[44]\tvalid_0's l2: 7838.12\n",
      "[45]\tvalid_0's l2: 7799.48\n",
      "[46]\tvalid_0's l2: 7769.26\n",
      "[47]\tvalid_0's l2: 7727.9\n",
      "[48]\tvalid_0's l2: 7706.92\n",
      "[49]\tvalid_0's l2: 7665.05\n",
      "[50]\tvalid_0's l2: 7623.8\n",
      "[51]\tvalid_0's l2: 7597.34\n",
      "[52]\tvalid_0's l2: 7573.94\n",
      "[53]\tvalid_0's l2: 7557.61\n",
      "[54]\tvalid_0's l2: 7512.07\n",
      "[55]\tvalid_0's l2: 7487.34\n",
      "[56]\tvalid_0's l2: 7467.39\n",
      "[57]\tvalid_0's l2: 7441.86\n",
      "[58]\tvalid_0's l2: 7411.44\n",
      "[59]\tvalid_0's l2: 7395.6\n",
      "[60]\tvalid_0's l2: 7369.6\n",
      "[61]\tvalid_0's l2: 7338.4\n",
      "[62]\tvalid_0's l2: 7320.22\n",
      "[63]\tvalid_0's l2: 7301.39\n",
      "[64]\tvalid_0's l2: 7279.27\n",
      "[65]\tvalid_0's l2: 7257.76\n",
      "[66]\tvalid_0's l2: 7242.74\n",
      "[67]\tvalid_0's l2: 7226.57\n",
      "[68]\tvalid_0's l2: 7153.47\n",
      "[69]\tvalid_0's l2: 7138.85\n",
      "[70]\tvalid_0's l2: 7129.01\n",
      "[71]\tvalid_0's l2: 7115.53\n",
      "[72]\tvalid_0's l2: 7101.32\n",
      "[73]\tvalid_0's l2: 7090.22\n",
      "[74]\tvalid_0's l2: 7077.72\n",
      "[75]\tvalid_0's l2: 7066.53\n",
      "[76]\tvalid_0's l2: 7047.6\n",
      "[77]\tvalid_0's l2: 7024.15\n",
      "[78]\tvalid_0's l2: 7013.73\n",
      "[79]\tvalid_0's l2: 6999.54\n",
      "[80]\tvalid_0's l2: 6919.55\n",
      "[81]\tvalid_0's l2: 6907.27\n",
      "[82]\tvalid_0's l2: 6894.65\n",
      "[83]\tvalid_0's l2: 6879.61\n",
      "[84]\tvalid_0's l2: 6871.58\n",
      "[85]\tvalid_0's l2: 6858.53\n",
      "[86]\tvalid_0's l2: 6848.76\n",
      "[87]\tvalid_0's l2: 6833.94\n",
      "[88]\tvalid_0's l2: 6825.95\n",
      "[89]\tvalid_0's l2: 6814.66\n",
      "[90]\tvalid_0's l2: 6804.5\n",
      "[91]\tvalid_0's l2: 6787.38\n",
      "[92]\tvalid_0's l2: 6769.15\n",
      "[93]\tvalid_0's l2: 6755.2\n",
      "[94]\tvalid_0's l2: 6745.69\n",
      "[95]\tvalid_0's l2: 6738.21\n",
      "[96]\tvalid_0's l2: 6720.63\n",
      "[97]\tvalid_0's l2: 6705.78\n",
      "[98]\tvalid_0's l2: 6694.08\n",
      "[99]\tvalid_0's l2: 6681.46\n",
      "[100]\tvalid_0's l2: 6667.69\n",
      "[101]\tvalid_0's l2: 6658.11\n",
      "[102]\tvalid_0's l2: 6651.78\n",
      "[103]\tvalid_0's l2: 6595.7\n",
      "[104]\tvalid_0's l2: 6585.3\n",
      "[105]\tvalid_0's l2: 6538.36\n",
      "[106]\tvalid_0's l2: 6526.41\n",
      "[107]\tvalid_0's l2: 6519.25\n",
      "[108]\tvalid_0's l2: 6504.03\n",
      "[109]\tvalid_0's l2: 6496.58\n",
      "[110]\tvalid_0's l2: 6481.4\n",
      "[111]\tvalid_0's l2: 6472.59\n",
      "[112]\tvalid_0's l2: 6467.35\n",
      "[113]\tvalid_0's l2: 6458.73\n",
      "[114]\tvalid_0's l2: 6455.71\n",
      "[115]\tvalid_0's l2: 6451.55\n",
      "[116]\tvalid_0's l2: 6442.18\n",
      "[117]\tvalid_0's l2: 6436.97\n",
      "[118]\tvalid_0's l2: 6419.8\n",
      "[119]\tvalid_0's l2: 6383.51\n",
      "[120]\tvalid_0's l2: 6381.33\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[120]\tvalid_0's l2: 6381.33\n",
      "Save model...\n",
      "Start predicting...\n",
      "The mean_absolute_error of prediction train_data is: 30.435494961\n",
      "The mean_absolute_error of prediction valid_data is: 41.9829920369\n"
     ]
    }
   ],
   "source": [
    "# lightgbm 算法预测\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "y_train = train_y.values\n",
    "y_test = test_y.values\n",
    "\n",
    "X_train = train_x[feature_imports].values\n",
    "X_test = test_x[feature_imports].values\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train,silent=True)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {'task': 'train',\n",
    "   'boosting_type': 'gbdt',\n",
    "   'objective': 'regression',\n",
    "   'metric':  'mse',\n",
    "   'num_leaves': 2**5,\n",
    "   'learning_rate': 0.1,\n",
    "   'feature_fraction': 0.9,\n",
    "   'bagging_fraction': 0.8,\n",
    "   'bagging_freq': 5,\n",
    "   'verbose': 50,\n",
    "    'verbose_eval':50}\n",
    "  \n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=120,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=10)\n",
    "\n",
    "print('Save model...')\n",
    "\n",
    "# save model to file\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(gbm,'gbm.pkl')\n",
    "\n",
    "clf = joblib.load(\"train_model.m\")\n",
    "\n",
    "gbm.save_model('lightgbm_model.txt')\n",
    "print('Start predicting...')\n",
    "\n",
    "\n",
    "# predict\n",
    "\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "vlid_pred = gbm.predict(valid_x[feature_imports], num_iteration=gbm.best_iteration)\n",
    "# eval print(y_pred)\n",
    "print('The mean_absolute_error of prediction train_data is:', mean_absolute_error(y_test, y_pred))\n",
    "print('The mean_absolute_error of prediction valid_data is:', mean_absolute_error(valid_y, vlid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存特征 预测使用特征，及预测的dt ,计算预测误差，分析特征重要性\n",
    "save_df = valid_x[feature_imports]\n",
    "save_df['pred_dt'] = valid_predict\n",
    "save_df.to_csv('20181011_lgb_vlid_pred0707.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后续暂未用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dump model to JSON...')\n",
    "\n",
    "# dump model to json (and save to file)\n",
    "model_json = gbm.dump_model()\n",
    "\n",
    "with open('lightgbm_model.json', 'w+') as f:\n",
    "    json.dump(model_json, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调参\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "下面几张表为重要参数的含义和如何应用\n",
    "\n",
    "\n",
    "max_depth 树的最大深度 当模型过拟合时,可以考虑首先降低 max_depth  \n",
    "min_data_in_leaf 叶子可能具有的最小记录数 默认20，过拟合时用 \n",
    "feature_fraction 例如 为0.8时，意味着在每次迭代中随机选择80％的参数来建树 boosting 为 random forest 时用 \n",
    "bagging_fraction 每次迭代时用的数据比例 用于加快训练速度和减小过拟合 \n",
    "early_stopping_round 如果一次验证数据的一个度量在最近的early_stopping_round 回合中没有提高，模型将停止训练 加速分析，减少过多迭代 \n",
    "lambda 指定正则化 0～1 \n",
    "min_gain_to_split 描述分裂的最小 gain 控制树的有用的分裂 \n",
    "max_cat_group 在 group 边界上找到分割点 当类别数量很多时，找分割点很容易过拟合时 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "使用学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Mar 31 21:19:09 2018\n",
    "\n",
    "@author: hello4720\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### 读取数据\n",
    "print(\"载入数据\")\n",
    "dataset1 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data1.csv')\n",
    "dataset2 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data2.csv')\n",
    "dataset3 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data3.csv')\n",
    "dataset4 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data4.csv')\n",
    "dataset5 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data5.csv')\n",
    "\n",
    "dataset1.drop_duplicates(inplace=True)\n",
    "dataset2.drop_duplicates(inplace=True)\n",
    "dataset3.drop_duplicates(inplace=True)\n",
    "dataset4.drop_duplicates(inplace=True)\n",
    "dataset5.drop_duplicates(inplace=True)\n",
    "\n",
    "### 数据合并\n",
    "print(\"数据合并\")\n",
    "trains = pd.concat([dataset1,dataset2],axis=0)\n",
    "trains = pd.concat([trains,dataset3],axis=0)\n",
    "trains = pd.concat([trains,dataset4],axis=0)\n",
    "\n",
    "online_test = dataset5\n",
    "\n",
    "### 数据拆分\n",
    "print(\"数据拆分\")\n",
    "train_xy,offline_test = train_test_split(trains, test_size = 0.2,random_state=21)\n",
    "train,val = train_test_split(train_xy, test_size = 0.2,random_state=21)\n",
    "\n",
    "print(\"训练集\")\n",
    "y = train.is_trade                                                  # 训练集标签\n",
    "X = train.drop(['instance_id','is_trade'],axis=1)                   # 训练集特征矩阵\n",
    "\n",
    "print(\"验证集\")\n",
    "val_y = val.is_trade                                                # 验证集标签\n",
    "val_X = val.drop(['instance_id','is_trade'],axis=1)                 # 验证集特征矩阵\n",
    "\n",
    "print(\"测试集\")\n",
    "offline_test_X=offline_test.drop(['instance_id','is_trade'],axis=1) # 线下测试特征矩阵\n",
    "online_test_X=online_test.drop(['instance_id'],axis=1)              # 线上测试特征矩阵\n",
    "\n",
    "### 数据转换\n",
    "lgb_train = lgb.Dataset(X, y, free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(val_X, val_y, reference=lgb_train,free_raw_data=False)\n",
    "\n",
    "### 开始训练\n",
    "print('设置参数')\n",
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'boosting': 'dart',\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "\n",
    "            'learning_rate': 0.01,\n",
    "            'num_leaves':25,\n",
    "            'max_depth':3,\n",
    "\n",
    "            'max_bin':10,\n",
    "            'min_data_in_leaf':8,\n",
    "\n",
    "            'feature_fraction': 0.6,\n",
    "            'bagging_fraction': 1,\n",
    "            'bagging_freq':0,\n",
    "\n",
    "            'lambda_l1': 0,\n",
    "            'lambda_l2': 0,\n",
    "            'min_split_gain': 0\n",
    "}\n",
    "\n",
    "print(\"开始训练\")\n",
    "gbm = lgb.train(params,                     # 参数字典\n",
    "                lgb_train,                  # 训练集\n",
    "                num_boost_round=2000,       # 迭代次数\n",
    "                valid_sets=lgb_eval,        # 验证集\n",
    "                early_stopping_rounds=30)   # 早停系数\n",
    "### 线下预测\n",
    "print (\"线下预测\")\n",
    "preds_offline = gbm.predict(offline_test_X, num_iteration=gbm.best_iteration) # 输出概率\n",
    "offline=offline_test[['instance_id','is_trade']]\n",
    "offline['preds']=preds_offline\n",
    "offline.is_trade = offline['is_trade'].astype(np.float64)\n",
    "print('log_loss', metrics.log_loss(offline.is_trade, offline.preds))\n",
    "\n",
    "### 线上预测\n",
    "print(\"线上预测\")\n",
    "preds_online =  gbm.predict(online_test_X, num_iteration=gbm.best_iteration)  # 输出概率\n",
    "online=online_test[['instance_id']]\n",
    "online['preds']=preds_online\n",
    "online.rename(columns={'preds':'predicted_score'},inplace=True)\n",
    "online.to_csv(\"./data/20180405.txt\",index=None,sep=' ')\n",
    "\n",
    "### 保存模型\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(gbm,'gbm.pkl')\n",
    "\n",
    "### 特征选择\n",
    "df = pd.DataFrame(X.columns.tolist(), columns=['feature'])\n",
    "df['importance']=list(gbm.feature_importance())\n",
    "df = df.sort_values(by='importance',ascending=False)\n",
    "df.to_csv(\"./data/feature_score_20180405.csv\",index=None,encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 导入模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "\n",
    "### 载入数据\n",
    "print('载入数据')\n",
    "dataset1 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data1.csv')\n",
    "dataset2 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data2.csv')\n",
    "dataset3 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data3.csv')\n",
    "dataset4 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data4.csv')\n",
    "dataset5 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data5.csv')\n",
    "\n",
    "print('数据去重')\n",
    "dataset1.drop_duplicates(inplace=True)\n",
    "dataset2.drop_duplicates(inplace=True)\n",
    "dataset3.drop_duplicates(inplace=True)\n",
    "dataset4.drop_duplicates(inplace=True)\n",
    "dataset5.drop_duplicates(inplace=True)\n",
    "\n",
    "print('数据合并')\n",
    "trains = pd.concat([dataset1,dataset2],axis=0)\n",
    "trains = pd.concat([trains,dataset3],axis=0)\n",
    "trains = pd.concat([trains,dataset4],axis=0)\n",
    "\n",
    "online_test = dataset5\n",
    "\n",
    "### 数据拆分(训练集+验证集+测试集)\n",
    "print('数据拆分')\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_xy,offline_test = train_test_split(trains,test_size = 0.2,random_state=21)\n",
    "train,val = train_test_split(train_xy,test_size = 0.2,random_state=21)\n",
    "\n",
    "# 训练集\n",
    "y_train = train.is_trade                                               # 训练集标签\n",
    "X_train = train.drop(['instance_id','is_trade'],axis=1)                # 训练集特征矩阵\n",
    "\n",
    "# 验证集\n",
    "y_val = val.is_trade                                                   # 验证集标签\n",
    "X_val = val.drop(['instance_id','is_trade'],axis=1)                    # 验证集特征矩阵\n",
    "\n",
    "# 测试集\n",
    "offline_test_X = offline_test.drop(['instance_id','is_trade'],axis=1)  # 线下测试特征矩阵\n",
    "online_test_X  = online_test.drop(['instance_id'],axis=1)              # 线上测试特征矩阵\n",
    "\n",
    "### 数据转换\n",
    "print('数据转换')\n",
    "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train,free_raw_data=False)\n",
    "\n",
    "### 设置初始参数--不含交叉验证参数\n",
    "print('设置参数')\n",
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric': 'binary_logloss',\n",
    "          }\n",
    "\n",
    "### 交叉验证(调参)\n",
    "print('交叉验证')\n",
    "min_merror = float('Inf')\n",
    "best_params = {}\n",
    "\n",
    "# 准确率\n",
    "print(\"调参1：提高准确率\")\n",
    "for num_leaves in range(20,200,5):\n",
    "    for max_depth in range(3,8,1):\n",
    "        params['num_leaves'] = num_leaves\n",
    "        params['max_depth'] = max_depth\n",
    "\n",
    "        cv_results = lgb.cv(\n",
    "                            params,\n",
    "                            lgb_train,\n",
    "                            seed=2018,\n",
    "                            nfold=3,\n",
    "                            metrics=['binary_error'],\n",
    "                            early_stopping_rounds=10,\n",
    "                            verbose_eval=True\n",
    "                            )\n",
    "\n",
    "        mean_merror = pd.Series(cv_results['binary_error-mean']).min()\n",
    "        boost_rounds = pd.Series(cv_results['binary_error-mean']).argmin()\n",
    "\n",
    "        if mean_merror < min_merror:\n",
    "            min_merror = mean_merror\n",
    "            best_params['num_leaves'] = num_leaves\n",
    "            best_params['max_depth'] = max_depth\n",
    "\n",
    "params['num_leaves'] = best_params['num_leaves']\n",
    "params['max_depth'] = best_params['max_depth']\n",
    "\n",
    "# 过拟合\n",
    "print(\"调参2：降低过拟合\")\n",
    "for max_bin in range(1,255,5):\n",
    "    for min_data_in_leaf in range(10,200,5):\n",
    "            params['max_bin'] = max_bin\n",
    "            params['min_data_in_leaf'] = min_data_in_leaf\n",
    "\n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=42,\n",
    "                                nfold=3,\n",
    "                                metrics=['binary_error'],\n",
    "                                early_stopping_rounds=3,\n",
    "                                verbose_eval=True\n",
    "                                )\n",
    "\n",
    "            mean_merror = pd.Series(cv_results['binary_error-mean']).min()\n",
    "            boost_rounds = pd.Series(cv_results['binary_error-mean']).argmin()\n",
    "\n",
    "            if mean_merror < min_merror:\n",
    "                min_merror = mean_merror\n",
    "                best_params['max_bin']= max_bin\n",
    "                best_params['min_data_in_leaf'] = min_data_in_leaf\n",
    "\n",
    "params['min_data_in_leaf'] = best_params['min_data_in_leaf']\n",
    "params['max_bin'] = best_params['max_bin']\n",
    "\n",
    "print(\"调参3：降低过拟合\")\n",
    "for feature_fraction in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    for bagging_fraction in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "        for bagging_freq in range(0,50,5):\n",
    "            params['feature_fraction'] = feature_fraction\n",
    "            params['bagging_fraction'] = bagging_fraction\n",
    "            params['bagging_freq'] = bagging_freq\n",
    "\n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=42,\n",
    "                                nfold=3,\n",
    "                                metrics=['binary_error'],\n",
    "                                early_stopping_rounds=3,\n",
    "                                verbose_eval=True\n",
    "                                )\n",
    "\n",
    "            mean_merror = pd.Series(cv_results['binary_error-mean']).min()\n",
    "            boost_rounds = pd.Series(cv_results['binary_error-mean']).argmin()\n",
    "\n",
    "            if mean_merror < min_merror:\n",
    "                min_merror = mean_merror\n",
    "                best_params['feature_fraction'] = feature_fraction\n",
    "                best_params['bagging_fraction'] = bagging_fraction\n",
    "                best_params['bagging_freq'] = bagging_freq\n",
    "\n",
    "params['feature_fraction'] = best_params['feature_fraction']\n",
    "params['bagging_fraction'] = best_params['bagging_fraction']\n",
    "params['bagging_freq'] = best_params['bagging_freq']\n",
    "\n",
    "print(\"调参4：降低过拟合\")\n",
    "for lambda_l1 in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    for lambda_l2 in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "        for min_split_gain in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "            params['lambda_l1'] = lambda_l1\n",
    "            params['lambda_l2'] = lambda_l2\n",
    "            params['min_split_gain'] = min_split_gain\n",
    "\n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=42,\n",
    "                                nfold=3,\n",
    "                                metrics=['binary_error'],\n",
    "                                early_stopping_rounds=3,\n",
    "                                verbose_eval=True\n",
    "                                )\n",
    "\n",
    "            mean_merror = pd.Series(cv_results['binary_error-mean']).min()\n",
    "            boost_rounds = pd.Series(cv_results['binary_error-mean']).argmin()\n",
    "\n",
    "            if mean_merror < min_merror:\n",
    "                min_merror = mean_merror\n",
    "                best_params['lambda_l1'] = lambda_l1\n",
    "                best_params['lambda_l2'] = lambda_l2\n",
    "                best_params['min_split_gain'] = min_split_gain\n",
    "\n",
    "params['lambda_l1'] = best_params['lambda_l1']\n",
    "params['lambda_l2'] = best_params['lambda_l2']\n",
    "params['min_split_gain'] = best_params['min_split_gain']\n",
    "\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "### 训练\n",
    "params['learning_rate']=0.01\n",
    "lgb.train(\n",
    "          params,                     # 参数字典\n",
    "          lgb_train,                  # 训练集\n",
    "          valid_sets=lgb_eval,        # 验证集\n",
    "          num_boost_round=2000,       # 迭代次数\n",
    "          early_stopping_rounds=50    # 早停次数\n",
    "          )\n",
    "\n",
    "### 线下预测\n",
    "print (\"线下预测\")\n",
    "preds_offline = lgb.predict(offline_test_X, num_iteration=lgb.best_iteration) # 输出概率\n",
    "offline=offline_test[['instance_id','is_trade']]\n",
    "offline['preds']=preds_offline\n",
    "offline.is_trade = offline['is_trade'].astype(np.float64)\n",
    "print('log_loss', metrics.log_loss(offline.is_trade, offline.preds))\n",
    "\n",
    "### 线上预测\n",
    "print(\"线上预测\")\n",
    "preds_online =  lgb.predict(online_test_X, num_iteration=lgb.best_iteration)  # 输出概率\n",
    "online=online_test[['instance_id']]\n",
    "online['preds']=preds_online\n",
    "online.rename(columns={'preds':'predicted_score'},inplace=True)           # 更改列名\n",
    "online.to_csv(\"./data/20180405.txt\",index=None,sep=' ')                   # 保存结果\n",
    "\n",
    "### 保存模型\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lgb,'lgb.pkl')\n",
    "\n",
    "### 特征选择\n",
    "df = pd.DataFrame(X_train.columns.tolist(), columns=['feature'])\n",
    "df['importance']=list(lgb.feature_importance())                           # 特征分数\n",
    "df = df.sort_values(by='importance',ascending=False)                      # 特征排序\n",
    "df.to_csv(\"./data/feature_score_20180331.csv\",index=None,encoding='gbk')  # 保存分数\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
